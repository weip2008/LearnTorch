{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor in PyTorch is a multi-dimensional array similar to NumPy arrays but with additional capabilities. Tensors are a fundamental data structure in PyTorch and are used to encode the inputs and outputs of a model, as well as the model's parameters. PyTorch tensors support automatic differentiation, which is essential for training deep learning models.\n",
    "\n",
    "### Key Features of PyTorch Tensors\n",
    "\n",
    "1. **Multi-dimensional Array**: Tensors can be 1-dimensional (vectors), 2-dimensional (matrices), or higher-dimensional arrays.\n",
    "2. **GPU Acceleration**: Tensors can be moved to and operated on a GPU to speed up computation.\n",
    "3. **Automatic Differentiation**: Tensors can track operations to enable automatic differentiation via PyTorch's autograd system, which is crucial for training neural networks.\n",
    "\n",
    "### Creating Tensors\n",
    "\n",
    "You can create tensors in several ways, such as from data, from NumPy arrays, or using built-in functions. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.1148, 0.3096, 0.6898],\n",
      "        [0.7177, 0.3627, 0.7386]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# From data\n",
    "data = [[1, 2], [3, 4]]\n",
    "tensor_from_data = torch.tensor(data)\n",
    "print(tensor_from_data)\n",
    "\n",
    "# From NumPy array\n",
    "import numpy as np\n",
    "np_array = np.array(data)\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "print(tensor_from_np)\n",
    "\n",
    "# Using built-in functions\n",
    "tensor_zeros = torch.zeros(2, 3)   # 2x3 matrix of zeros\n",
    "tensor_ones = torch.ones(2, 3)     # 2x3 matrix of ones\n",
    "tensor_random = torch.rand(2, 3)   # 2x3 matrix with random values\n",
    "print(tensor_zeros)\n",
    "print(tensor_ones)\n",
    "print(tensor_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "You can perform a variety of operations on tensors, including arithmetic operations, indexing, slicing, and more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "tensor(1)\n",
      "tensor([2, 5, 8])\n",
      "tensor([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "tensor_a = torch.tensor([1.0, 2.0, 3.0])\n",
    "tensor_b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise addition\n",
    "tensor_sum = tensor_a + tensor_b\n",
    "print(tensor_sum)\n",
    "\n",
    "# Element-wise multiplication\n",
    "tensor_mul = tensor_a * tensor_b\n",
    "print(tensor_mul)\n",
    "\n",
    "# Matrix multiplication\n",
    "tensor_c = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_d = torch.tensor([[5, 6], [7, 8]])\n",
    "tensor_matmul = torch.matmul(tensor_c, tensor_d)\n",
    "print(tensor_matmul)\n",
    "\n",
    "# Indexing and slicing\n",
    "tensor_e = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(tensor_e[0, 0])   # Accessing the first element\n",
    "print(tensor_e[:, 1])   # Accessing the second column\n",
    "print(tensor_e[1, :])   # Accessing the second row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Tensors to GPU\n",
    "\n",
    "If you have a GPU available, you can move tensors to the GPU to accelerate computation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor_a.to('cuda')\n",
    "    print(tensor_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Tensors with the attribute `requires_grad=True` will track operations on them. When you call `.backward()`, the gradient of these tensors will be calculated and stored in the `.grad` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor with requires_grad=True\n",
    "tensor_f = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "result = tensor_f * 2 # each element multiply by 2 which is the gradient\n",
    "result_sum = result.sum()\n",
    "\n",
    "# Compute gradients\n",
    "result_sum.backward()\n",
    "\n",
    "# Display gradients\n",
    "print(tensor_f.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëçüòÑ **Conclusion**: In summary, PyTorch tensors are versatile, support GPU acceleration, and are fundamental for building and training neural networks due to their ability to track gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
