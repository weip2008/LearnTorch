R-squared (R²), also known as the coefficient of determination, is a statistical measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is commonly used in the context of regression analysis to assess the goodness of fit of a model.

### Key Points About R²:

1. **Range**:
   - R² ranges from 0 to 1.
   - An R² of 0 means that the independent variables do not explain any of the variability of the dependent variable.
   - An R² of 1 means that the independent variables explain all the variability of the dependent variable.

2. **Interpretation**:
   - A higher R² value indicates a better fit between the model and the observed data.
   - For example, an R² of 0.8 suggests that 80% of the variance in the dependent variable is explained by the independent variables.

3. **Calculation**:
   - R² is calculated as the square of the correlation coefficient (r) in simple linear regression.
   - In the context of multiple regression, it is the ratio of the sum of squares of the regression (SSR) to the total sum of squares (SST): \( R^2 = \frac{SSR}{SST} \).

4. **Limitations**:
   - R² does not indicate whether a regression model is appropriate; it simply measures how well the model explains the variance of the dependent variable.
   - It can be misleading with models that do not include a constant term or with data that have non-linear relationships.

5. **Adjusted R²**:
   - Adjusted R² adjusts the statistic based on the number of predictors in the model, providing a more accurate measure when comparing models with different numbers of predictors.

### Example:
In a linear regression model predicting house prices based on square footage, an R² of 0.75 would mean that 75% of the variability in house prices can be explained by square footage. The remaining 25% is due to other factors not included in the model.

### Sources:
- [Statistics How To](https://www.statisticshowto.com/probability-and-statistics/coefficient-of-determination-r-squared/)
- [Investopedia](https://www.investopedia.com/terms/r/r-squared.asp)
- [Khan Academy](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/regression-library/v/r-squared-or-coefficient-of-determination)

This understanding helps in determining how well your model is performing and guides you in improving or selecting models for predictive analytics.

In the context of regression analysis, SSR stands for **Sum of Squares for Regression**. It is a measure used to quantify the amount of variation in the dependent variable that is explained by the independent variable(s) in the model.

### Calculation of SSR:

SSR is calculated by summing the squares of the differences between the predicted values (from the regression model) and the mean of the dependent variable. Mathematically, it is represented as:

\[ SSR = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 \]

where:
- \( \hat{y}_i \) = the predicted value for the i-th observation,
- \( \bar{y} \) = the mean of the observed values of the dependent variable,
- \( n \) = the number of observations.

### Interpretation:

- **High SSR**: A high SSR indicates that a significant portion of the total variation in the dependent variable is explained by the regression model.
- **Low SSR**: A low SSR suggests that the model does not explain much of the variation in the dependent variable.

### Relation to Total Sum of Squares (SST) and Sum of Squares for Error (SSE):

The Total Sum of Squares (SST) represents the total variation in the dependent variable and is the sum of SSR and the Sum of Squares for Error (SSE):

\[ SST = SSR + SSE \]

- **SST**: Total Sum of Squares, which measures the total variation in the dependent variable.
- **SSE**: Sum of Squares for Error, which measures the variation in the dependent variable that is not explained by the model.

### Example:

Suppose you have a simple linear regression model predicting a student's test score (Y) based on the number of study hours (X). The SSR would quantify how much of the variation in test scores is explained by the number of study hours.

### Sources:
- [Statistics How To - Sum of Squares: SST, SSR, SSE](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/sum-squares/)
- [Investopedia - Sum of Squares](https://www.investopedia.com/terms/s/sum-of-squares.asp)
- [Khan Academy - ANOVA 1: Calculating SST, SSR, and SSE](https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library/ANOVA1/v/ssr-sst-sse)

Understanding SSR helps in evaluating the performance of a regression model and determining how well the independent variables explain the variability in the dependent variable.

In the context of regression analysis, SST stands for **Total Sum of Squares**. It is a measure of the total variability in the dependent variable (Y) and is used to quantify the overall variance that exists in the data.

### Calculation of SST:

SST is calculated by summing the squares of the differences between each observed value and the mean of the observed values. Mathematically, it is represented as:

\[ SST = \sum_{i=1}^{n} (y_i - \bar{y})^2 \]

where:
- \( y_i \) = the observed value for the i-th observation,
- \( \bar{y} \) = the mean of the observed values of the dependent variable,
- \( n \) = the number of observations.

### Interpretation:

- **SST** represents the total variability in the dependent variable (Y).
- It provides a baseline measure of how much the values of the dependent variable deviate from the mean.
- In the context of regression analysis, SST is decomposed into two parts: the variation explained by the regression model (SSR) and the variation not explained by the model (SSE).

### Relation to SSR and SSE:

SST can be broken down into the Sum of Squares for Regression (SSR) and the Sum of Squares for Error (SSE):

\[ SST = SSR + SSE \]

- **SSR (Sum of Squares for Regression)**: Measures the variation in the dependent variable that is explained by the regression model.
- **SSE (Sum of Squares for Error)**: Measures the variation in the dependent variable that is not explained by the model.

### Example:

Consider a simple linear regression model predicting a student's test score based on the number of study hours. SST would measure the total variability in test scores among all students, regardless of their study hours.

### Sources:

- [Statistics How To - Sum of Squares: SST, SSR, SSE](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/sum-squares/)
- [Investopedia - Sum of Squares](https://www.investopedia.com/terms/s/sum-of-squares.asp)
- [Khan Academy - ANOVA 1: Calculating SST, SSR, and SSE](https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library/ANOVA1/v/ssr-sst-sse)

Understanding SST helps in assessing the total variation in the dependent variable, which can then be compared to the variation explained by the regression model to determine its effectiveness.

The Total Sum of Squares (SST) and the Sum of Squares for Regression (SSR) are both key components in the analysis of variance in regression models. Here’s a detailed comparison of the two:

### Definition:

1. **SST (Total Sum of Squares)**:
   - **Definition**: SST measures the total variation in the dependent variable (Y) around its mean (\(\bar{Y}\)).
   - **Formula**: 
     \[
     SST = \sum_{i=1}^{n} (Y_i - \bar{Y})^2
     \]
     where \(Y_i\) is the observed value and \(\bar{Y}\) is the mean of observed values.

2. **SSR (Sum of Squares for Regression)**:
   - **Definition**: SSR measures the variation in the dependent variable that is explained by the regression model.
   - **Formula**: 
     \[
     SSR = \sum_{i=1}^{n} (\hat{Y}_i - \bar{Y})^2
     \]
     where \(\hat{Y}_i\) is the predicted value and \(\bar{Y}\) is the mean of observed values.

### Purpose:

1. **SST**:
   - Measures the total variability in the dependent variable, providing a baseline measure of how much the values of the dependent variable deviate from the mean.
   - **Usage**: Used to understand the total variability in the dataset.

2. **SSR**:
   - Measures the part of the total variability that is explained by the independent variables in the regression model.
   - **Usage**: Used to determine how well the regression model explains the variation in the dependent variable.

### Components of SST:

The SST is divided into two parts:
\[ SST = SSR + SSE \]

1. **SSR (Sum of Squares for Regression)**: Variation explained by the regression model.
2. **SSE (Sum of Squares for Error)**: Variation not explained by the regression model (residuals).

### Interpretation:

1. **SST**:
   - A larger SST indicates more total variation in the dependent variable.
   - It does not provide information about the model’s explanatory power.

2. **SSR**:
   - A larger SSR indicates that the regression model explains a significant portion of the total variation.
   - A higher SSR relative to SST suggests a better fit of the model.

### Example:

Consider a regression model predicting students' test scores based on study hours.

- **SST**: Represents the total variation in test scores among all students.
- **SSR**: Represents the variation in test scores that can be explained by the number of study hours.

### Conclusion:

While SST gives a measure of the total variability in the data, SSR helps understand how much of that variability can be attributed to the model's predictors. Together with SSE, these measures help evaluate the performance and fit of the regression model.

### References:

- [Statistics How To - Sum of Squares: SST, SSR, SSE](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/sum-squares/)
- [Investopedia - Sum of Squares](https://www.investopedia.com/terms/s/sum-of-squares.asp)
- [Khan Academy - ANOVA 1: Calculating SST, SSR, and SSE](https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library/ANOVA1/v/ssr-sst-sse)





