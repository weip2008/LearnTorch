<h1>PyTorch Learning Notes</h1>

```
ğŸ‘ğŸ˜„ äº‹ç‰©å¯ä»¥é¢„æµ‹ï¼Œæ˜¯å› ä¸ºäº‹ç‰©æœ¬èº«éµå¾ªä¸€å®šçš„å®¢è§‚è§„å¾‹ã€‚
Things can be predicted because things themselves follow certain objective laws.
```
> å› æœå…³ç³»å’Œç›¸å…³æ€§æ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µï¼š
>ã€å› æœå…³ç³»ã€‘ï¼ˆCausationï¼‰ï¼šæŒ‡çš„æ˜¯ä¸€ç§äº‹ä»¶æˆ–è¡Œä¸ºå¯¼è‡´å¦ä¸€ç§äº‹ä»¶æˆ–è¡Œä¸ºå‘ç”Ÿçš„å…³ç³»ã€‚åœ¨å› æœå…³ç³»ä¸­ï¼Œä¸€ä¸ªäº‹ä»¶è¢«è®¤ä¸ºæ˜¯å¦ä¸€ä¸ªäº‹ä»¶çš„åŸå› ï¼Œå› æ­¤ï¼Œå®ƒä»¬ä¹‹é—´å­˜åœ¨ç€æ—¶é—´ä¸Šçš„é¡ºåºå’Œå› æœè”ç³»ã€‚å› æœå…³ç³»é€šå¸¸è¢«æè¿°ä¸ºâ€œå¦‚æœ...é‚£ä¹ˆ...â€çš„å½¢å¼ï¼Œå³å¦‚æœå‘ç”Ÿäº‹ä»¶Aï¼Œåˆ™äº‹ä»¶Bå°†ä¼šå‘ç”Ÿã€‚
>ã€ç›¸å…³æ€§ã€‘ï¼ˆCorrelationï¼‰ï¼šæŒ‡çš„æ˜¯ä¸¤ä¸ªæˆ–å¤šä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå½“ä¸€ä¸ªå˜é‡å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¦ä¸€ä¸ªå˜é‡ä¹Ÿå¯èƒ½éšä¹‹å‘ç”Ÿå˜åŒ–ã€‚ç„¶è€Œï¼Œç›¸å…³æ€§å¹¶ä¸æ„å‘³ç€å…¶ä¸­ä¸€ä¸ªå˜é‡çš„å˜åŒ–æ˜¯å¦ä¸€ä¸ªå˜é‡å˜åŒ–çš„åŸå› ã€‚ç›¸å…³æ€§åªæ˜¯æè¿°äº†å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸æä¾›æœ‰å…³è¿™ç§å…³ç³»çš„åŸå› çš„ä¿¡æ¯ã€‚
å› æ­¤ï¼Œå°½ç®¡ç›¸å…³æ€§å¯ä»¥æ˜¾ç¤ºå‡ºä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦ï¼Œä½†è¦ç¡®è®¤ä¸€ä¸ªå˜é‡æ˜¯å¦å¯¼è‡´å¦ä¸€ä¸ªå˜é‡å˜åŒ–ï¼Œé€šå¸¸éœ€è¦æ›´å¤šçš„ç ”ç©¶å’Œè¯æ®æ¥ç¡®å®šå› æœå…³ç³»æ˜¯å¦å­˜åœ¨ã€‚å› æœå…³ç³»çš„ç¡®è®¤éœ€è¦æ’é™¤å…¶ä»–å¯èƒ½çš„è§£é‡Šï¼Œå¹¶ç¡®ä¿æ‰€è§‚å¯Ÿåˆ°çš„å…³è”æ˜¯çœŸæ­£çš„å› æœè”ç³»ï¼Œè€Œä¸æ˜¯ç®€å•çš„ç›¸å…³æ€§ã€‚

å¦‚æœæœ‰10ä¸ªåŸå› é€ æˆä¸€ä¸ªç»“æœï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªçŸ¥é“é€ æˆç»“æœçš„ä¸€éƒ¨åˆ†åŸå› ï¼Œè€Œä¸æ˜¯å…¨éƒ¨åŸå› ã€‚å¦‚æœæˆ‘ä»¬åªçŸ¥é“å…¶ä¸­6ä¸ªåŸå› ï¼Œé‚£ä¹ˆè¿™äº›åŸå› å’Œç»“æœä¹‹é—´å¯èƒ½å­˜åœ¨å› æœå…³ç³»ï¼Œä¹Ÿå¯èƒ½å­˜åœ¨ç›¸å…³æ€§ï¼Œæˆ–è€…ä¸¤è€…å…¼æœ‰ã€‚

ã€å› æœå…³ç³»ã€‘ï¼šå¦‚æœè¿™6ä¸ªå·²çŸ¥çš„åŸå› ä¸­çš„æŸäº›å› ç´ ç¡®å®æ˜¯å¯¼è‡´ç»“æœå‘ç”Ÿçš„åŸå› ï¼Œå¹¶ä¸”å…¶ä»–æœªçŸ¥çš„åŸå› ä¸ä¼šæ”¹å˜è¿™ä¸ªç»“æœï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¯´å­˜åœ¨å› æœå…³ç³»ã€‚
ã€ç›¸å…³æ€§ã€‘ï¼šå³ä½¿æˆ‘ä»¬ä¸çŸ¥é“å…¨éƒ¨çš„åŸå› ï¼Œè¿™6ä¸ªå·²çŸ¥çš„åŸå› å¯èƒ½ä¸ç»“æœä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™6ä¸ªå› ç´ çš„å˜åŒ–å¯èƒ½ä¼šä¸ç»“æœçš„å˜åŒ–ç›¸å…³è”ï¼Œå³ä½¿å®ƒä»¬ä¸æ˜¯ç›´æ¥çš„åŸå› ã€‚
å› æ­¤ï¼Œæ ¹æ®æˆ‘ä»¬å¯¹åŸå› çš„äº†è§£ç¨‹åº¦ä»¥åŠè¿™äº›å› ç´ ä¸ç»“æœä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè¯´è¿™äº›åŸå› ä¸ç»“æœä¹‹é—´å­˜åœ¨ä¸€å®šç¨‹åº¦çš„å› æœå…³ç³»æˆ–ç›¸å…³æ€§ã€‚

<font style="background-color:yellow">å³ä½¿æˆ‘ä»¬åªèƒ½æä¾›å…¶ä¸­çš„ä¸€éƒ¨åˆ†å› ç´ ï¼ŒAIæ¨¡å‹ä¹Ÿå¯ä»¥å°è¯•é¢„æµ‹è‚¡ç¥¨çš„æ¶¨è·Œã€‚</font>æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½æ¨¡å‹èƒ½å¤Ÿä»æä¾›çš„æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè¶‹åŠ¿ï¼Œå¹¶è¯•å›¾é¢„æµ‹æœªæ¥çš„è‚¡ç¥¨ä»·æ ¼å˜åŠ¨ã€‚ç„¶è€Œï¼Œè¿™ç§é¢„æµ‹çš„å‡†ç¡®æ€§å–å†³äºå¤šç§å› ç´ ï¼ŒåŒ…æ‹¬æä¾›çš„æ•°æ®è´¨é‡ã€æ¨¡å‹çš„å¤æ‚æ€§ã€è®­ç»ƒæ•°æ®çš„æ—¶é—´è·¨åº¦å’Œå¯ç”¨æ€§ç­‰ã€‚

åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè‚¡ç¥¨å¸‚åœºå—åˆ°è®¸å¤šä¸ç¡®å®šå› ç´ çš„å½±å“ï¼Œå¦‚ç»æµçŠ¶å†µã€æ”¿æ²»äº‹ä»¶ã€è‡ªç„¶ç¾å®³ç­‰ï¼Œè¿™äº›å› ç´ å¯èƒ½ä¼šä½¿è‚¡ç¥¨ä»·æ ¼çš„å˜åŠ¨å˜å¾—å¤æ‚å’Œéš¾ä»¥é¢„æµ‹ã€‚å› æ­¤ï¼Œå³ä½¿ä½¿ç”¨äº†AIæ¨¡å‹ï¼Œå¯¹è‚¡ç¥¨ä»·æ ¼çš„å‡†ç¡®é¢„æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚

â“ Factors affecting stock market?
* [Factors affecting stock market](https://www.adityabirlacapital.com/abc-of-money/factors-affecting-stock-market#2)
> 1. Supply and demand
> 2. Company related factors
> 3. Investor sentiment (What kind of sentiment of investors?)
> 4. Interest rates
> 5. Politics
> 6. Current events
> 7. Natural calamities
> 8. Exchange rates

## Getting Started

1. set local virtual environment (env)
python -m venv env
2. pip install torch

## Errors
â“ not available module
![](images/unavailable%20modue.png)
ğŸ“ close VS Code, reopen, and run all code from the top.

## Linear Modeling

* [generate linear model with noise](../src/genLinear.py)
  $$y=f(x)=3\cdot x - 7 + noise$$
* [read data from file and plot it](../src/plotLinear.py)
![](images/noiseLinearData.png)
* [create a linear model based on data](../src/linearModel.py)
![](images/lr=0.01.png)
![](images/epoch=20.png)

ğŸ‘ğŸ˜„ **Conclusion**
1. linear model does NOT give exactly we expected.
2. lr (learning rate) is kind of sensitive
3. epoch make difference


### Model class

```py
# Step 2: Build and Train the Neural Network Model
# Define the neural network model
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)
    
    def forward(self, x):
        return self.linear(x)
```
![](images/LinearModel.png)


### Training process
```mermaid
graph LR

TRAIN(train data)
MODEL(create a model)
TRAINING[Training Process]
TRAIN_OUTPUT(expected Train<br>Output)
TRAINED_MODEL(Trained Model)

TEST(test data)
TEST_OUTPUT(expected Test<br>Output)
TESTING[Testing Process]
ACCURACY[Modeling Accuracy]

REAL(Real Data)
PRED[Predictation process]
RESULT[Prediction <br>Result]

TRAIN --> TRAINING
TRAIN_OUTPUT --> TRAINING
MODEL --> TRAINING
TRAINING ==> TRAINED_MODEL

TEST --> TESTING
TRAINED_MODEL --> TESTING
TEST_OUTPUT --> TESTING
TESTING ==> ACCURACY

TRAINED_MODEL --> PRED
REAL --> PRED
PRED ==> RESULT
ACCURACY --How good it is--> RESULT

classDef start fill:#3cdf77,stroke:#1a6d38,stroke-width:2px,color:white;
classDef html fill:#F46624,stroke:#F46624,stroke-width:4px,color:white;
classDef js fill:#73dbf7,stroke:#194652,stroke-width:2px;
classDef if fill:#f2e589,stroke:black,stroke-width:2px;
classDef db fill:#aaafb0,stroke:#1a404a,stroke-width:2px;
classDef end1 fill:#f17168,stroke:#902a23,stroke-width:2px,color:white;

class TRAIN,TRAIN_OUTPUT,MODEL,TEST,TRAINED_MODEL,TEST_OUTPUT,REAL start
class TRAINING,TESTING,PRED js
class ACCURACY,RESULT html
```

## AI on Fashion
![sample fashion images](images/fashionSample.png)
![](images/neuralNetwork4handwritingDigits.png)
28X28=784 input, 2 modle layer,  0-9 output
$$f_{l+1} = \sigma (w_l a_l + b_l) $$
$w_l$: weight for layer l
$b_l$: bias for layer l
$\sigma$: activation function
$f_{l+1}$: l+1 function of layer l
the purpose of modeling is find each $w_l$ and $b_l$
$$f=\sum_{l=0}^{l=n} {f_{l}}$$

### Activation Function



* [tensor basics](torchBasics.ipynb)

* [Load data from network, Understand image data, and squeeze(), transpose()](../src/fashion01.py)
* [create model based on all images, and save model into a file](../src/fashion02.py)
* [load model from file, and predict a given image](../src/fashion03.py)


## Homework
* create model for handwriting digits.
```py
train_dataset = datasets.MNIST('data/MNIST_data/', download=True, train=True, transform=ToTensor())
test_data = datasets.MNIST('data/MNIST_data/', download=True, train=False, transform=ToTensor())

```

* [Understand weight in linear function](../src/weight.py)
* [Understand ReLU activate function](../src/relu.py)
* [efact only on x<0](../src/ReLU1.py)
* [efact only on x<0](../src/ReLU2.py)

## Linear Regression
* [Prepare linear data](../src/linear01.py)
* [understand epoch, ir, batch_size, optimizer, loss function, and modeling](../src/linear02.py)
* [manually simulate model creation process](../src/linear03.py)
* [create model based on network data](../src/linear04.py)

## Non-linear Function Fit

$$f(x) = x^3 + \frac 1 2 x^2 - 4 x -2$$

* [generate data based on above function](../src/nonlinear01.py)
* [get 2 pints on the nonlinear function, make a stright line based on the 2 points](../src/nonlinear02.py)
* [generate data based on above function](../src/nonlinear03.py)
* [use activate function to get part of the line](../src/nonlinear04.py)
* [save our model to file relu_model.pth](../src/nonlinear05.py)
* [use relu activate function to train our model](../src/nonlinear06.py)
* [save our model to file relu_model.pth](../src/nonlinear07.py)
* [what are weights and bias look like](../src/nonlinear08.py)

## backpropagation
* [wikipedia Backpropagation Explain](https://en.wikipedia.org/wiki/Backpropagation)
$$y_j=relu\left( \sum_{k=1}^n w_{kj}\cdot x_{k} + b_j\right)$$
wher relu() is activation function, and $y_j$ is layer j output.

## Translator Project

```mermaid
graph LR

ENG(English audio)
ENG_TXT[English text]
CHN_TXT[Chinese text]
CHN[Chinese audio]

ENG --> ENG_TXT -->CHN_TXT --> CHN

classDef start fill:#3cdf77,stroke:#1a6d38,stroke-width:2px,color:white;
classDef js fill:#73dbf7,stroke:#194652,stroke-width:2px;
classDef end1 fill:#f17168,stroke:#902a23,stroke-width:2px,color:white;

class ENG start
class CHN end1
class ENG_TXT,CHN_TXT js
```
* [load wav audio from internet](../src/audio/audio01.py)